{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f13dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv\n",
    "%pip install -U langchain langchain-google-genai langchain-tavily python-dotenv aiosqlite\n",
    "%pip install --upgrade langgraph\n",
    "%pip install langgraph-checkpoint-sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5902172f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import operator\n",
    "from typing import Annotated, List, Any, Dict\n",
    "from dataclasses import dataclass, field\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, ToolMessage,AIMessage, BaseMessage, AnyMessage\n",
    "from datetime import date\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_tavily import TavilySearch\n",
    "import uuid\n",
    "\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "import sqlite3\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv('GEMINI_API_KEY')\n",
    "os.environ['TAVILY_API_KEY'] = os.getenv('TAVILY_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "187a9bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]\n",
    "\n",
    "conn = sqlite3.connect(\"checkpoints.db\", check_same_thread=False)\n",
    "memory = SqliteSaver(conn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13e4309e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, model, tools, system=\"\", checkpointer=None):\n",
    "        self.system = system\n",
    "        \n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_gemini)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\"llm\", self.exists_action, {True: \"action\", False: END})\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile(\n",
    "            checkpointer=checkpointer,\n",
    "            interrupt_before=[\"action\"]\n",
    "        ) \n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "        \n",
    "    def call_gemini(self, state: AgentState):\n",
    "        messages = state['messages']\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages          \n",
    "        message = self.model.invoke(messages)\n",
    "        return {'messages': [message]}\n",
    "    \n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state['messages'][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "    \n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            result = self.tools[t['name']].invoke(t['args'])\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "        print(\"Voltando para o modelo!\")\n",
    "        return {'messages': results}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "04c3ecd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
     ]
    }
   ],
   "source": [
    "current_date = date.today().strftime(\"%d/%m/%Y\")\n",
    "tool = TavilySearch(max_results=3)  # usa TAVILY_API_KEY do ambiente\n",
    "prompt = f\"\"\"Você é um assistente de pesquisa inteligente e altamente atualizado. \\\n",
    "Sua principal prioridade é encontrar as informações mais RECENTES e em TEMPO REAL sempre que possível. \\\n",
    "A data atual é {current_date}. \\\n",
    "Ao buscar sobre o tempo ou eventos que se referem a \"hoje\" ou \"agora\", \\\n",
    "você DEVE **incluir a data atual `{current_date}` na sua consulta para a ferramenta de busca**. \\\n",
    "Por exemplo, se a pergunta é \"tempo em cidade x hoje\", a consulta para a ferramenta deve ser \"tempo em cidade x {current_date}\". \\\n",
    "Ignore ou descarte informações que claramente se refiram a datas passadas ou futuras ao responder perguntas sobre \"hoje\". \\\n",
    "Use o mecanismo de busca para procurar informações, sempre buscando o \"hoje\" ou o \"agora\" quando o contexto indicar. \\\n",
    "Você tem permissão para fazer múltiplas chamadas (seja em conjunto ou em sequência). \\\n",
    "Procure informações apenas quando tiver certeza do que você quer. \\\n",
    "Se precisar pesquisar alguma informação antes de fazer uma pergunta de acompanhamento, você tem permissão para fazer isso!\n",
    "\"\"\"\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "abot = Agent(model, [tool], system = prompt, checkpointer = memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03355a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meu novo Thread ID dinâmico é: 459a720f-95db-4c04-8a13-926c52be8e38\n"
     ]
    }
   ],
   "source": [
    "dynamic_thread_id = str(uuid.uuid4())\n",
    "\n",
    "print(f\"Meu novo Thread ID dinâmico é: {dynamic_thread_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "537a8ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Iniciando nova conversa com ID: 90d17b54-2edc-42fc-8d80-ef32393b132d\n",
      "\n",
      "\n",
      "--- Etapa 1: Agente processa a entrada e decide a ação ---\n",
      "Você: Como está o tempo em São Paulo hoje (21/08/2025)?\n",
      "\n",
      "\n",
      "Agente (decisão): \n",
      "\n",
      " --- AGENTE PAUSADO: Intervenção Humana Necessaria ---\n",
      "\n",
      "O agente decidiu executar a(s) seguinte(s) ação(ões) de ferramenta:\n",
      "- Ferramenta: tavily_search, Argumentos: {'query': 'tempo em São Paulo 21/08/2025', 'search_depth': 'basic'}\n",
      "\n",
      "--- Etapa2: Remotando a execução (Agente executaré a ação) ---\n",
      "Voltando para o modelo!\n",
      "Para 21/08/2025 em São Paulo, as previsões do tempo são um pouco variadas:\n",
      "\n",
      "*   Algumas fontes indicam céu nublado com pancadas rápidas de chuva, com temperaturas entre 16°C e 32°C.\n",
      "*   Outras preveem um dia ensolarado, com temperaturas mínimas entre 19°C e 20.1°C e máximas entre 32.1°C e 33°C, sem previsão de chuva e com umidade do ar em torno de 23%.\n",
      "\n",
      "Devido às informações conflitantes, não é possível dar uma previsão exata.\n",
      "--- FIM DA INTERAÇÃO ---\n"
     ]
    }
   ],
   "source": [
    "def extract_text(msg):\n",
    "    if isinstance(msg.content, list):\n",
    "        return \"\".join(\n",
    "            part.get(\"text\", \"\")\n",
    "            for part in msg.content\n",
    "            if part.get(\"type\") == \"text\"\n",
    "        )\n",
    "    return msg.content\n",
    "\n",
    "session_id = str(uuid.uuid4())\n",
    "print(f'DEBUG: Iniciando nova conversa com ID: {session_id}\\n')\n",
    "\n",
    "user_mesage = \"Como está o tempo em São Paulo hoje (21/08/2025)?\"\n",
    "messages = [HumanMessage(content=user_mesage)]\n",
    "thread_config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "\n",
    "print(\"\\n--- Etapa 1: Agente processa a entrada e decide a ação ---\")\n",
    "print(f\"Você: {user_mesage}\")\n",
    "\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread_config): \n",
    "\n",
    "    for k, v in event.items(): \n",
    "        if k == \"llm\":\n",
    "            last_message = v[\"messages\"][-1]\n",
    "            print(extract_text(last_message))\n",
    "            if isinstance (last_message, AIMessage) and last_message.tool_calls: \n",
    "                print(f'\\nAgente (decisão): {extract_text(last_message)}') \n",
    "                print(\"\\n --- AGENTE PAUSADO: Intervenção Humana Necessaria ---\") \n",
    "            else: \n",
    "                print(f'\\nAgente (resposta direta/sem tool_cals): {extract_text(last_message)}') \n",
    "                print(\"\\n --- AGENTE PAUSADO: (resposta direta, sem ação pendente) ---\") \n",
    "current_state = abot.graph.get_state(thread_config)\n",
    "\n",
    "last_state_message = current_state.values['messages'][-1]\n",
    "\n",
    "if current_state and current_state.next == ('action',) and isinstance (last_state_message, AIMessage) and last_state_message.tool_calls:\n",
    "    tool_calls_pending = last_state_message.tool_calls\n",
    "    if tool_calls_pending:\n",
    "        print('\\nO agente decidiu executar a(s) seguinte(s) ação(ões) de ferramenta:')\n",
    "        for tc in tool_calls_pending:\n",
    "            print(f'- Ferramenta: {tc[\"name\"]}, Argumentos: {tc[\"args\"]}')\n",
    "\n",
    "        user_input = input('\\nVocê deseja que o agente execute esta(s) ação(ões)? (sim/não)').lower()\n",
    "\n",
    "        if user_input == 'sim':\n",
    "            print('\\n--- Etapa2: Remotando a execução (Agente executaré a ação) ---')\n",
    "            for event in abot.graph.stream(None, thread_config):\n",
    "                for k, v in event.items():\n",
    "                    if k == \"llm\":\n",
    "                        last_message = v[\"messages\"][-1]\n",
    "                        print(extract_text(last_message))\n",
    "                    elif k == 'llm':\n",
    "                        final_response_message = v.get('messages', [])[-1].content\n",
    "                        print(f'\\n Agente (resposta final): {extract_text(v[\"messages\"][-1])}')\n",
    "                    elif k == END:\n",
    "                        print(f'DEBUG: Grafo terminou a execução.')\n",
    "            print(\"--- FIM DA INTERAÇÃO ---\")\n",
    "        else:\n",
    "            print(\"Execução cancelada pelo usuário\")\n",
    "            print(\"--- FIM DA INTERAÇÃO ---\")\n",
    "    else:\n",
    "        print(\"\\nO Agente não decidiu nenhuma ação de ferramenta apesar da pausa. Interação encerrada.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nO Agente respondeu diretamente ou não pausou nenhuma ação. Não há ações pendentes para aprovar.\")\n",
    "    if current_state:\n",
    "        final_response_message = current_state.values['messages'][-1].content\n",
    "        print(f'Agente (resposta direta): {final_response_message.content[0][\"text\"]}')\n",
    "    print(\"--- FIM DA INTERAÇÃO ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
